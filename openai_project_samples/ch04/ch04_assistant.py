# ìŒì„± ë…¹ìŒì„ ìœ„í•œ ì˜¤ë””ì˜¤ ë ˆì½”ë” íŒ¨í‚¤ì§€ ì¶”ê°€
from audiorecorder import audiorecorder

# ìŠ¤íŠ¸ë¦¼ë¦¿ íŒ¨í‚¤ì§€ ì¶”ê°€
import streamlit as st

# OpenAI íŒ¨í‚¤ì§€ ì¶”ê°€
from openai import OpenAI

#íŒŒì´ì¬ ê¸°ë³¸ íŒ¨í‚¤ì§€
import os
import base64
import numpy as np

# ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” STT API
def STT(audio, client) :
  # Whisper APIê°€ íŒŒì¼ í˜•íƒœë¡œ ìŒì„±ì„ ì…ë ¥ë°›ìœ¼ë¯€ë¡œ input.mp3 íŒŒì¼ì„ ì €ì¥
  filename = 'input.mp3'
  wav_file = open(filename, "wb")
  wav_file.write(audio.export().read())
  wav_file.close()

  # ìŒì„± íŒŒì¼ ì½ê¸°
  audio_file = open(filename, "rb")

  try:
    transcript = client.audio.transcriptions.create(
      model="whisper-1",
      file=audio_file,
      response_format="text"
    )

    audio_file.close()
    os.remove(filename)
  except:
    transcript='API Keyë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”'
  return transcript

def TTS(response, client):
  # TTSë¥¼ í™œìš©í•˜ì—¬ ë§Œë“  ìŒì„±ì„ íŒŒì¼ë¡œ ì €ì¥
  with client.audio.speech.with_streaming_response.create(
    model="tts-1",
    voice="onyx",
    input=response
  ) as response:
    filename = "output.mp3"
    response.stream_to_file(filename)
  
  # ì €ì¥í•œ ìŒì„± íŒŒì¼ì„ ìë™ ì¬ìƒ
  with open(filename, "rb") as f:
    data = f.read()
    b64 = base64.b64encode(data).decode()

    # ìŠ¤íŠ¸ë¦¼ë¦¿ì—ì„œ ìŒì„± ìë™ ì¬ìƒ
    md = f"""
      <audio autoplay="True">
      <source src="data:audio/mp3;base64,{b64}" type="audio/mp3">
      </audio>
    """

    st.markdown(md, unsafe_allow_html=True,)

  os.remove(filename)  

def ask_gpt(prompt, client):
  response = client.chat.completions.create(
    model='gpt-3.5-turbo',
    messages=prompt
  )
  return response.choices[0].message.content

def main():
  client = OpenAI(api_key='yourServiceKey')

  st.set_page_config(
    page_title='ìŒì„± ë¹„ì„œ í”„ë¡œê·¸ë¨ğŸ”Š',
    layout='wide'
  )

  if "check_audio" not in st.session_state:
    st.session_state["check_audio"] = []

  if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role":"system", "content":"You are a thoughtful assistant. Respond to all input in 25 words and answer in korean"}]

  col1, col2 = st.columns(2)

  with col1:
    st.header('AI Assistant ğŸ”Š')
    st.image('ai.png', width=200)
    st.markdown('---')

    flag_start = False
    
    audio = audiorecorder('ì§ˆë¬¸', 'ë…¹ìŒì¤‘...')
    if len(audio) > 0 and not np.array_equal(audio, st.session_state['check_audio']):
      st.audio(audio.export().read())

      question = STT(audio, client)

      st.session_state["messages"] = st.session_state["messages"] + [{"role":"user", "content": question}]
      st.session_state["check_audio"] = audio
      flag_start=True

  with col2:
    st.subheader('ëŒ€í™”ê¸°ë¡ âŒ¨')
    if flag_start:

      response = ask_gpt(st.session_state["messages"], client)
      st.session_state["messages"] = st.session_state["messages"] + [{"role":"assistant", "content": response}]

      for message in st.session_state["messages"]:
        if message["role"] != 'system':
          with st.chat_message(message['role']):
            st.markdown(message["content"])
      
      TTS(response, client)

if __name__=='__main__':
  main()